{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "threshold = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 10780437\n"
     ]
    }
   ],
   "source": [
    "\n",
    "current_directory = os.getcwd()\n",
    "filename = \"wiki2.train.txt\"\n",
    "\n",
    "path = os.path.join(current_directory, filename)\n",
    "text = open(path).read().lower()\n",
    "\n",
    "print('Corpus length:', len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 2088629\n"
     ]
    }
   ],
   "source": [
    "# parse my corpus into tokens\n",
    "tokens = text.split(' ')\n",
    "print(\"Tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 2088629\n"
     ]
    }
   ],
   "source": [
    "#reduce vocabulary size by setting\n",
    "def replace_low_frequency_tokens(tokens, threshold):\n",
    "    # Count the frequency of each token\n",
    "    token_counts = Counter(tokens)\n",
    "\n",
    "    # Replace tokens with frequency less than the threshold\n",
    "    replaced_tokens = ['<unk>' if token_counts[token] < threshold else token for token in tokens]\n",
    "\n",
    "    return replaced_tokens \n",
    "\n",
    "result_tokens = replace_low_frequency_tokens(tokens, threshold)\n",
    "print(\"Tokens:\", len(result_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from string tokens to integers\n",
    "def map_tokens(tokens):\n",
    "    token_to_int = {}\n",
    "    next_int = 0\n",
    "    for token in tokens:\n",
    "        if token not in token_to_int:\n",
    "            # Assign the next available integer to the token\n",
    "            token_to_int[token] = next_int\n",
    "            next_int += 1\n",
    "    return token_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of my dictionary 20488\n",
      "Second key: \n",
      "\n",
      "Second value: 1\n"
     ]
    }
   ],
   "source": [
    "token_to_int = map_tokens(result_tokens)\n",
    "\n",
    "# Convert dictionary items to a list and get the second item\n",
    "second_item = list(token_to_int.items())[1]\n",
    "# Get the length of the dictionary\n",
    "dict_length = len(token_to_int)\n",
    "print(\"the length of my dictionary\",dict_length )\n",
    "# Unpack the key-value pair\n",
    "second_key, second_value = second_item\n",
    "\n",
    "# Print the result\n",
    "print(\"Second key:\", second_key)\n",
    "print(\"Second value:\", second_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first key-value pair\n",
    "def embedded_tokens(tokens):\n",
    "    token_to_int = map_tokens(tokens)\n",
    "    # Convert string tokens to corresponding integers\n",
    "    integer_tokens = [token_to_int[token] for token in tokens]\n",
    "    # Convert the list of integers to a PyTorch tensor\n",
    "    tensor_tokens = torch.LongTensor(integer_tokens)\n",
    "    print(tensor_tokens.size)\n",
    "    # Define the embedding layer\n",
    "    embedding_dim = 100\n",
    "    embedding_layer = nn.Embedding(len(token_to_int), embedding_dim)\n",
    "    # Apply the embedding layer to the tensor tokens\n",
    "    embedded_tokens = embedding_layer(tensor_tokens)\n",
    "    return embedded_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x2a4dabcf0>\n",
      "tensor([[-1.0645, -0.5560,  1.1581,  0.2047,  0.7313, -0.1741, -1.4433,  0.2520,\n",
      "         -0.7750, -0.6832, -0.5626, -0.0383, -0.9045,  1.3169,  0.9567, -0.1529,\n",
      "          0.4737, -2.1305, -0.0195,  0.0875, -0.1613, -0.7381, -1.0351,  0.3427,\n",
      "         -2.2066, -0.5151,  0.7881, -0.4703,  1.6502,  0.4971,  0.5738, -0.7173,\n",
      "          0.3770, -0.9329, -0.4933, -0.2825,  0.4503, -1.2146, -0.3587,  0.3481,\n",
      "         -0.5452,  1.6961, -0.4438,  0.0902, -1.0837,  1.2130, -0.4440,  0.5164,\n",
      "         -1.4188,  2.7317,  0.7299, -0.2487, -0.2830, -0.4773,  1.5749, -0.3705,\n",
      "         -0.5113,  0.7672, -1.4359, -0.1097,  1.1370, -0.3071,  0.2373, -0.8157,\n",
      "          0.4613, -0.1003, -0.5933, -2.0658, -0.2865, -0.4746,  0.4497, -0.7076,\n",
      "         -0.5849, -1.3941, -0.7461, -0.0638, -1.3246, -0.4170,  1.4104, -1.4733,\n",
      "         -0.6330,  0.4995, -0.9390, -1.0378,  2.1513, -1.4099, -2.4437, -2.4021,\n",
      "          2.0527,  0.0771,  0.0841, -2.2044, -1.3055, -1.4154,  1.0789,  1.3719,\n",
      "          0.5976,  0.3010, -0.5243, -0.2057],\n",
      "        [ 0.3618,  0.0258,  0.6698, -0.9929,  1.2605, -0.1477, -1.4818,  0.2193,\n",
      "          0.8560, -0.4931,  0.4469,  0.3979, -1.5010,  0.1725,  1.1807,  1.1270,\n",
      "          0.6203,  0.9856, -0.9897, -1.3372,  0.3885, -1.8363, -0.8464, -0.6836,\n",
      "         -1.2696,  0.9048,  1.0612,  0.0439,  0.8074,  1.9084, -0.2545,  1.3486,\n",
      "         -0.6761,  0.5719, -1.4462, -1.2689,  0.4461,  1.2993,  1.5049, -1.9292,\n",
      "          1.3645, -0.2881, -0.1855,  1.1620, -0.6420,  0.9317, -0.7679,  2.0039,\n",
      "         -0.5620, -1.1636, -0.3323, -0.1103,  0.8369, -0.2664,  2.5290,  0.5693,\n",
      "          1.7817,  1.4478, -1.2957,  0.3783,  0.3923, -0.5651,  0.7390,  0.2356,\n",
      "         -1.2995,  1.0997,  0.4181,  0.4408,  0.3496,  0.4150, -0.4925,  1.6250,\n",
      "         -1.4608, -1.4638,  0.1447,  1.2974, -3.7034,  0.4067, -0.1369, -0.8232,\n",
      "          0.2584, -1.3178,  0.4715,  0.3044, -0.5699, -1.0974, -2.0524, -1.1479,\n",
      "          0.9106, -0.2080, -0.0900,  0.6229,  1.6626, -1.1631,  0.5345,  0.0592,\n",
      "         -0.4328,  0.0405, -0.0628,  1.9861],\n",
      "        [ 0.0289, -0.3159, -1.4845,  0.4529, -1.6817,  1.7112, -1.9579, -0.7926,\n",
      "         -0.2029,  0.3443, -0.4315, -0.1069, -3.0370,  0.4671,  0.3769,  0.6817,\n",
      "          0.6868,  1.4485, -0.3450, -0.6597,  0.0977, -1.6353,  1.2428, -1.3255,\n",
      "         -0.1090,  0.8803,  1.7705, -0.2319,  0.9172, -1.2604,  1.1126,  0.5985,\n",
      "          0.1235,  1.7822,  0.7540, -1.2431,  0.6174, -0.9489, -0.2364, -0.3858,\n",
      "         -0.5286, -1.2322,  1.8068, -1.0537,  0.8312, -1.4918, -0.8155, -2.1133,\n",
      "         -0.5784,  0.5897, -0.0711, -0.5692,  1.1800, -0.9776,  0.5151, -0.8512,\n",
      "          0.5526,  0.6390, -0.5202, -1.7480, -0.4740, -1.8149, -0.2037, -0.3387,\n",
      "          1.4813, -0.7757,  0.1186,  0.3138,  1.7319,  0.2368, -1.2351,  1.2798,\n",
      "         -1.3262, -0.0784,  0.5561, -0.7030,  0.1126,  0.2915, -0.0914, -0.4134,\n",
      "          0.0977,  0.5622,  0.1614,  1.4235, -2.8150, -0.6811,  1.2474,  0.2298,\n",
      "         -1.9108,  0.9319, -0.3580,  0.2155,  0.7990,  1.1096,  0.7845, -0.7977,\n",
      "          0.8036,  1.8621, -1.7562,  1.0709]], grad_fn=<SliceBackward0>)\n",
      "torch.Size([2088629, 100])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedded_tokens = embedded_tokens(tokens)\n",
    "# Extract a sample (e.g., first three tokens)\n",
    "sample_size = 3\n",
    "sampled_tokens = embedded_tokens[:sample_size]\n",
    "print(sampled_tokens)\n",
    "print(embedded_tokens.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_size, hidden_size, num_layers, batch_size, input_sequence):\n",
    "    # Create an RNN layer\n",
    "    rnn_layer = nn.RNN(input_size, hidden_size, num_layers)\n",
    "    # Initialize the hidden state\n",
    "    hidden_state = torch.zeros(num_layers, batch_size, hidden_size)  \n",
    "    # Forward pass through the RNN layer\n",
    "    output_sequence, final_hidden_state = rnn_layer(input_sequence, hidden_state) \n",
    "    return output_sequence      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx should also be 2-D but got 3-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedded_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[76], line 7\u001b[0m, in \u001b[0;36mmodel\u001b[0;34m(input_size, hidden_size, num_layers, batch_size, input_sequence)\u001b[0m\n\u001b[1;32m      5\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(num_layers, batch_size, hidden_size)  \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Forward pass through the RNN layer\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m output_sequence, final_hidden_state \u001b[38;5;241m=\u001b[39m \u001b[43mrnn_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_sequence\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/rnn.py:530\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m hx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 530\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    531\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor unbatched 2-D input, hx should also be 2-D but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    532\u001b[0m         hx \u001b[38;5;241m=\u001b[39m hx\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx should also be 2-D but got 3-D tensor"
     ]
    }
   ],
   "source": [
    "input_size = 5\n",
    "hidden_size = 10\n",
    "num_layers = 2\n",
    "batch_size = 64\n",
    "model(input_size, hidden_size, num_layers, batch_size, embedded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
